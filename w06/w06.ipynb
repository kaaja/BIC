{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 Last week we used the activation function. Why is this not used with backpropagation?\n",
    "\n",
    "__A:__ The function is not differentiable.\n",
    "\n",
    "# 2 What is the minimum number of hidden neuron layers needed in order to approximate an arbitrary continuous function, and why?\n",
    "\n",
    "__A:__ One hidden layer is the minimum. \"Universal approximation theorem: Any continuous function\n",
    "can be approximated by a neural network with a single\n",
    "hidden layer\", slide 54 https://www.uio.no/studier/emner/matnat/ifi/INF3490/h18/timeplan/slides/lecture6-1pp.pdf\n",
    "\n",
    "\n",
    "Without a hidden layer, it is impossible to approximate non-linear separable problems. The first layer gives lines, and the second layer opens up the dimension to two, so e.g. traingles can be made. Furthermore, a compbination of enough traingles can approximate any shape. See slide 12 https://www.uio.no/studier/emner/matnat/ifi/INF3490/h18/timeplan/slides/lecture6-1pp.pdf\n",
    "\n",
    "<mark> UNCLEAR <br>\n",
    "    \n",
    "    \n",
    "    \n",
    "# 3 Why do we use a validation set? Describe how the three different cross-validation methods presented in the lecture slides work, and what their advantages and disadvantages are.\n",
    "We want the model to generalize well, meaning that it works well on other data than the data the model was estimated on. When fitting the model to the full dataset, the model will typically not fit well for other data. In oredr to create a model that generalizes well, the data is split into a training and a validation set. The model is fit to the training set, and is then evaluated on the validation set. The model performance on the validation set describes how well the model generalizes. <br>\n",
    "\n",
    "Three crossvalidation methods. <br> \n",
    "1) Split into training and validation (and possible test). Works as the previous paragraph. <br>\n",
    "2) K-fold cross-validation. Split into K number of folds. Each fold performs as validation set once, while at the same time the model is fit to the remaining folds. <br>\n",
    "3) Bootstrapping. Draw random observations with replacement and use the rest as validation. Repeat.\n",
    "\n",
    "# 4 Implement the MLP shown below, and train it to correctly perform the XOR function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nn.weightMatrices [array([[ 1., -1.,  0.],\n",
      "       [ 1.,  0.,  1.]]), array([[ 1.,  1.,  0.],\n",
      "       [ 1., -1.,  1.]])]\n",
      "nn.weightedInputs [array([1., 2.]), array([2., 2.])]\n",
      "nn.outputs [array([1., 2.]), array([2., 2.])]\n",
      "nn.error2 5.0\n",
      "self.deltaVectors [array([1., 2.])]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'deltaVectors' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-112-c91365d0bd69>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'nn.error2'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merror2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'nn.deltaVectors'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeltaVectors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'weightMatrices'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweightMatrices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-112-c91365d0bd69>\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     57\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0moutputNumber\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m                     \u001b[0mdeltaSum\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeltaVectors\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mlayerNumber\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0moutputNumber\u001b[0m\u001b[0;34m]\u001b[0m                     \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweightMatrices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0moutputNumber\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mhiddenNodeNumber\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m# Check order indices\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m                 \u001b[0mdeltaVectors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minsert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdeltaSum\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactivationFunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweightedInputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m# Avoid hard coding later\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m                 \u001b[0;31m#print('len(deltaVectors[-2-layerNumber])', len(self.deltaVectors[-2-layerNumber]))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'deltaVectors' is not defined"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "class NN:\n",
    "    \"\"\" Multi layer. Single input\"\"\"\n",
    "    \n",
    "    def __init__(self, nodeNumbers, activationFunction, learningRate, targets, inputs, errorTolerance, test=False):\n",
    "        self.nodeNumbers, self.activationFunction, self.learningRate, self.targets, self.inputs, self.errorTolerance  \\\n",
    "        = nodeNumbers, activationFunction, learningRate, targets, inputs, errorTolerance\n",
    "        \n",
    "        self.weightMatrices = []\n",
    "        self.weightedInputs = []\n",
    "        self.outputs = []\n",
    "        self.deltaVectors = []\n",
    "        \n",
    "        \n",
    "        if not test:\n",
    "            for nodeNumber in range(len(nodeNumbers)-1):\n",
    "                self.weightMatrices.append(np.random.random_sample((nodeNumbers[nodeNumber+1], \\\n",
    "                                                                    nodeNumbers[nodeNumber+1] +1)) - .5)\n",
    "                self.weightedInputs.append(np.zeros(nodeNumbers[nodeNumber+1]))\n",
    "                self.outputs.append(np.zeros(nodeNumbers[nodeNumber+1]))\n",
    "                self.deltaVectors.append(np.zeros(nodeNumbers[nodeNumber]+1))\n",
    "            \n",
    "        else:\n",
    "            self.weightMatrices.append(np.array(((1., -1., 0.), (1., 0., 1.))))\n",
    "            self.weightMatrices.append(np.array(((1., 1., 0.), (1., -1., 1.)))) # Assumed same weights bias nodes both layers\n",
    "            self.inputs = np.array((1, 0, 1))\n",
    "\n",
    "        \n",
    "    def differentiateActivationFunction(self):\n",
    "        return 1\n",
    "    \n",
    "    def forward(self):\n",
    "        self.weightedInputs.append(self.weightMatrices[0] @ self.inputs)\n",
    "        self.outputs.append(self.activationFunction(self.weightedInputs[-1]))\n",
    "\n",
    "        for layer in range(1, len(self.nodeNumbers)-1):\n",
    "            outputsIncludingBias = np.concatenate([[1], self.outputs[-1]])\n",
    "            self.weightedInputs.append(self.weightMatrices[layer] @ outputsIncludingBias)\n",
    "            self.outputs.append(self.activationFunction(self.weightedInputs[-1]))\n",
    "            \n",
    "    def calculateErrors(self):\n",
    "        self.error2 = 0\n",
    "        for outputNodeNumber in range(len(self.outputs)):\n",
    "            self.error2 += (self.targets[outputNodeNumber] - \\\n",
    "                           self.outputs[-1][outputNodeNumber])**2\n",
    "        \n",
    "    def backward(self):\n",
    "        deltaValues = np.array([(self.outputs[-1][outputNumber] - self.targets[outputNumber])\\\n",
    "                      *self.differentiateActivationFunction() \\\n",
    "                      for outputNumber in range(len(self.targets))])\n",
    "        self.deltaVectors.append(deltaValues)\n",
    "        \n",
    "        outputsIncludingBias = np.concatenate([[1], self.outputs[-2]])\n",
    "        for hiddenNodeNumber in range(self.nodeNumbers[-2] + 1):\n",
    "            for outputNumber in range(len(self.targets)):\n",
    "                self.weightMatrices[-1][outputNumber, hiddenNodeNumber] -= \\\n",
    "                self.learningRate*self.deltaVectors[0][outputNumber]*outputsIncludingBias[hiddenNodeNumber]\n",
    "        \n",
    "        #deltaVec = np.zeros((2,2)) # HERE\n",
    "        for layerNumber in range(len(self.nodeNumbers)-2):\n",
    "            for hiddenNodeNumber in range(self.nodeNumbers[-1-(layerNumber+1)] + 1):\n",
    "                deltaSum = 0\n",
    "                print('self.deltaVectors', self.deltaVectors)\n",
    "                for outputNumber in range(len(self.targets)):\n",
    "                    deltaSum += self.deltaVectors[-1-layerNumber][outputNumber] \\\n",
    "                    * self.weightMatrices[-1][outputNumber,hiddenNodeNumber] # Check order indices\n",
    "                deltaVectors.insert(0, deltaSum*self.activationFunction(self.weightedInputs[0]))# Avoid hard coding later\n",
    "                #print('len(deltaVectors[-2-layerNumber])', len(self.deltaVectors[-2-layerNumber]))\n",
    "        \n",
    "def activationFunction(x):\n",
    "    return x\n",
    "        \n",
    "numberOfNodes,  activationFunction, learningRate, targets, inputs, errorTolerance = \\\n",
    "[2,2,2], activationFunction, 0.1, [1, 0], [0, 1], 1e-6\n",
    "test = True\n",
    "nn=NN(numberOfNodes, activationFunction, learningRate, targets, inputs, test, errorTolerance)\n",
    "print('nn.weightMatrices',nn.weightMatrices)\n",
    "\n",
    "nn.forward()\n",
    "print('nn.weightedInputs',nn.weightedInputs)\n",
    "print('nn.outputs',nn.outputs)\n",
    "\n",
    "nn.calculateErrors()\n",
    "print('nn.error2',nn.error2)\n",
    "\n",
    "nn.backward()\n",
    "print('nn.deltaVectors',nn.deltaVectors)\n",
    "print('weightMatrices', nn.weightMatrices)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
