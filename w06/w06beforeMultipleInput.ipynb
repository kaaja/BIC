{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 Last week we used the activation function. Why is this not used with backpropagation?\n",
    "\n",
    "__A:__ The function is not differentiable.\n",
    "\n",
    "# 2 What is the minimum number of hidden neuron layers needed in order to approximate an arbitrary continuous function, and why?\n",
    "\n",
    "__A:__ One hidden layer is the minimum. \"Universal approximation theorem: Any continuous function\n",
    "can be approximated by a neural network with a single\n",
    "hidden layer\", slide 54 https://www.uio.no/studier/emner/matnat/ifi/INF3490/h18/timeplan/slides/lecture6-1pp.pdf\n",
    "\n",
    "\n",
    "Without a hidden layer, it is impossible to approximate non-linear separable problems. The first layer gives lines, and the second layer opens up the dimension to two, so e.g. traingles can be made. Furthermore, a compbination of enough traingles can approximate any shape. See slide 12 https://www.uio.no/studier/emner/matnat/ifi/INF3490/h18/timeplan/slides/lecture6-1pp.pdf\n",
    "\n",
    "<mark> UNCLEAR <br>\n",
    "    \n",
    "    \n",
    "    \n",
    "# 3 Why do we use a validation set? Describe how the three different cross-validation methods presented in the lecture slides work, and what their advantages and disadvantages are.\n",
    "We want the model to generalize well, meaning that it works well on other data than the data the model was estimated on. When fitting the model to the full dataset, the model will typically not fit well for other data. In oredr to create a model that generalizes well, the data is split into a training and a validation set. The model is fit to the training set, and is then evaluated on the validation set. The model performance on the validation set describes how well the model generalizes. <br>\n",
    "\n",
    "Three crossvalidation methods. <br> \n",
    "1) Split into training and validation (and possible test). Works as the previous paragraph. <br>\n",
    "2) K-fold cross-validation. Split into K number of folds. Each fold performs as validation set once, while at the same time the model is fit to the remaining folds. <br>\n",
    "3) Bootstrapping. Draw random observations with replacement and use the rest as validation. Repeat.\n",
    "\n",
    "# 4 Implement the MLP shown below, and train it to correctly perform the XOR function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "self.weightMatrices [array([[ 1., -1.,  0.],\n",
      "       [ 1.,  0.,  1.]]), array([[ 1.,  1.,  0.],\n",
      "       [ 1., -1.,  1.]])]\n",
      "Iterations:  6 |Error|:  nan Output:  [nan nan]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/k/.local/lib/python3.6/site-packages/ipykernel_launcher.py:81: RuntimeWarning: overflow encountered in double_scalars\n",
      "/home/k/.local/lib/python3.6/site-packages/ipykernel_launcher.py:57: RuntimeWarning: overflow encountered in multiply\n",
      "/home/k/.local/lib/python3.6/site-packages/ipykernel_launcher.py:57: RuntimeWarning: overflow encountered in double_scalars\n",
      "/home/k/.local/lib/python3.6/site-packages/ipykernel_launcher.py:113: RuntimeWarning: invalid value encountered in double_scalars\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "class NN:\n",
    "    \"\"\" 2 layer (1 hidden). Single input combination of 2 intergers. Output 2 integers\"\"\"\n",
    "    \n",
    "    def __init__(self, nodeNumbers, activationFunction, learningRate, targets, inputs, errorTolerance, \\\n",
    "                 maxIterations, test=False):\n",
    "        self.nodeNumbers, self.activationFunction, self.learningRate, self.targets, self.inputs, self.errorTolerance,  \\\n",
    "        self.maxIterations, self.test= nodeNumbers, activationFunction, learningRate, targets, inputs, errorTolerance,\\\n",
    "        maxIterations, test\n",
    "        \n",
    "        self.weightMatrices = []\n",
    "        \n",
    "        if not test:\n",
    "            for nodeNumber in range(len(nodeNumbers)-1):\n",
    "                self.weightMatrices.append(np.random.random_sample((nodeNumbers[nodeNumber+1], \\\n",
    "                                                                    nodeNumbers[nodeNumber+1] +1)) - .5)\n",
    "            print('self.weightMatrices', self.weightMatrices)\n",
    "            \n",
    "        else:\n",
    "            self.weightMatrices.append(np.array(((1., -1., 0.), (1., 0., 1.))))\n",
    "            self.weightMatrices.append(np.array(((1., 1., 0.), (1., -1., 1.)))) # Assumed same weights bias nodes both layers\n",
    "            self.inputs = np.array((1, 0, 1))\n",
    "            print('self.weightMatrices', self.weightMatrices)\n",
    "            \n",
    "    def run(self):\n",
    "        self.forward()\n",
    "        self.calculateErrors()\n",
    "        self.iterations = 1\n",
    "        self.outputForPlot1 = []\n",
    "        self.outputForPlot2 = []\n",
    "        while np.abs(self.error2) > self.errorTolerance and self.iterations < self.maxIterations: \n",
    "            self.backward()\n",
    "            self.forward()\n",
    "            #print('Output: ', self.outputs)\n",
    "            self.calculateErrors()\n",
    "            #print('self.error2',self.error2)\n",
    "\n",
    "            self.iterations += 1\n",
    "            self.outputForPlot1.append(self.outputs[-1][0])\n",
    "            self.outputForPlot2.append(self.outputs[-1][1])\n",
    "        #self.plot()\n",
    "\n",
    "        print('Iterations: ', self.iterations, '|Error|: ', self.error2, 'Output: ', self.outputs[1])\n",
    "        \n",
    "    def plot(self):\n",
    "        fig, (ax, ax2) = plt.subplots(1,2)\n",
    "        ax.plot(np.arange(len(self.outputForPlot1)), self.outputForPlot1)#, label='Input 1')\n",
    "        ax2.plot(np.arange(len(self.outputForPlot2)), self.outputForPlot2)#, label='Input 2')\n",
    "        ax.set_title('Input 1')\n",
    "        ax2.set_title('Input 22')\n",
    "        plt.savefig('simple.pdf')\n",
    "        \n",
    "    def differentiateActivationFunction(self, weightedInputs):\n",
    "        #print('weightedInputs ',weightedInputs)\n",
    "        if isinstance(weightedInputs, np.ndarray):\n",
    "            if weightedInputs.ndim == 1:\n",
    "                onesArray = np.ones(len(weightedInputs))\n",
    "        else:\n",
    "            onesArray = 1\n",
    "        out = self.activationFunction(weightedInputs)*(onesArray - self.activationFunction(weightedInputs))\n",
    "        #print(self.activationFunction(weightedInputs), onesArray, out)\n",
    "        #print('out ', out)\n",
    "        #return 1\n",
    "        return out\n",
    "        \n",
    "        \n",
    "    \n",
    "    def forward(self):\n",
    "        weightedInputs = []\n",
    "        weightedInputs.append(self.weightMatrices[0] @ self.inputs)\n",
    "        self.outputs = []\n",
    "        self.outputs.append(self.activationFunction(weightedInputs[-1]))\n",
    "\n",
    "        for layer in range(1, len(self.nodeNumbers)-1):\n",
    "            outputsIncludingBias = np.concatenate([[1], self.outputs[-1]])\n",
    "            weightedInputs.append(self.weightMatrices[layer] @ outputsIncludingBias)\n",
    "            self.outputs.append(self.activationFunction(weightedInputs[-1]))\n",
    "        self.weightedInputs = weightedInputs\n",
    "        #print('self.outputs', self.outputs[-1])\n",
    "            \n",
    "    def calculateErrors(self):\n",
    "        self.error2 = 0\n",
    "        for outputNodeNumber in range(len(self.outputs)):\n",
    "            self.error2 += (self.targets[outputNodeNumber] - \\\n",
    "                           self.outputs[-1][outputNodeNumber])**2\n",
    "        \n",
    "    def backward(self):\n",
    "        # Output deltas\n",
    "        self.deltaVectors = []\n",
    "        '''\n",
    "        deltaValuesOld = np.array([(self.outputs[-1][outputNumber] - self.targets[outputNumber])\\\n",
    "                      *self.differentiateActivationFunction(self.weightedInputs[-1][outputNumber]) \\\n",
    "                      for outputNumber in range(len(self.targets))])\n",
    "        '''\n",
    "        deltaValues = (self.outputs[-1] - self.targets)* self.differentiateActivationFunction(self.weightedInputs[-1])\n",
    "        self.deltaVectors.append(deltaValues)\n",
    "        \n",
    "        # Hidden deltas\n",
    "        for layerNumber in range(len(self.nodeNumbers)-2):\n",
    "            deltaVecs = []\n",
    "            for j in range(self.nodeNumbers[-1-(layerNumber+1)+1]):\n",
    "                deltaSum = 0\n",
    "                for k in range(len(self.targets)):\n",
    "                    deltaSum += self.deltaVectors[-1-layerNumber][k] * self.weightMatrices[-1][k,j+1] \n",
    "                deltaVecs.append(deltaSum*self.differentiateActivationFunction(self.weightedInputs[-1][j]))\n",
    "        self.deltaVectors.insert(0, deltaVecs)\n",
    "        \n",
    "        # Output weights\n",
    "        outputsIncludingBias = np.concatenate([[1], self.outputs[-2]])\n",
    "        for j in range(self.nodeNumbers[-2] + 1):\n",
    "            for k in range(len(self.targets)):\n",
    "                self.weightMatrices[-1][k, j] -= \\\n",
    "                self.learningRate*self.deltaVectors[1][k]*outputsIncludingBias[j]\n",
    "        \n",
    "        # Hidden weights\n",
    "        for inputNumber in range(self.nodeNumbers[0]+1):\n",
    "            for hiddenNodeNumber in range(self.nodeNumbers[1]): # fix indexes\n",
    "                self.weightMatrices[0][hiddenNodeNumber, inputNumber] -= self.learningRate \\\n",
    "                * self.deltaVectors[0][hiddenNodeNumber]*self.inputs[inputNumber]\n",
    "     \n",
    "\n",
    " \n",
    "\n",
    "def activationFunction(x):\n",
    "    return 1./(1+np.exp(-x))\n",
    "\n",
    "def activationFunction(x):\n",
    "    return x\n",
    " \n",
    "\n",
    "numberOfNodes,  activationFunction, learningRate, targets, inputs, errorTolerance, maxIterations = \\\n",
    "[2,2,2], activationFunction, 0.1, [1, 0], [0, 1], 1e-6, 10000\n",
    "test = True\n",
    "nn=NN(numberOfNodes, activationFunction, learningRate, targets, inputs, errorTolerance, maxIterations, test)\n",
    "'''\n",
    "print('nn.weightMatrices',nn.weightMatrices)\n",
    "\n",
    "nn.forward()\n",
    "print('nn.weightedInputs',nn.weightedInputs)\n",
    "print('nn.outputs',nn.outputs)\n",
    "\n",
    "nn.calculateErrors()\n",
    "print('nn.error2',nn.error2)\n",
    "\n",
    "nn.backward()\n",
    "print('nn.deltaVectors',nn.deltaVectors)\n",
    "print('weightMatrices', nn.weightMatrices)\n",
    "'''\n",
    "nn.run()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "self.weightMatrices [array([[-0.08851358,  0.47923393, -0.01478144],\n",
      "       [ 0.26991866, -0.12081351,  0.10372475]]), array([[ 0.19270413,  0.37050541,  0.16655994],\n",
      "       [-0.47359476, -0.29878165,  0.08109803]])]\n",
      "Iterations:  36 |Error|:  7.936851581157607e-07 Output:  [ 9.99576883e-01 -7.84000802e-04]\n"
     ]
    }
   ],
   "source": [
    "def activationFunction(x):\n",
    "    return x\n",
    "\n",
    "numberOfNodes,  activationFunction, learningRate, targets, inputs, errorTolerance, maxIterations = \\\n",
    "[2,2,2], activationFunction, 0.1, [1, 0], np.array((1, 0, 1)), 1e-6, 100\n",
    "test = False\n",
    "\n",
    "\n",
    "nn2=NN(numberOfNodes, activationFunction, learningRate, targets, inputs, errorTolerance, maxIterations, test)\n",
    "nn2.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "self.weightMatrices [array([[-0.02629585, -0.02179738,  0.16925185],\n",
      "       [ 0.23853403,  0.37057834,  0.28275949]]), array([[-0.24233045,  0.39173689,  0.09455386],\n",
      "       [ 0.37015428,  0.12036227,  0.44036432]])]\n",
      "Iterations:  100000 |Error|:  5.042944549922127e-05 Output:  [0.99498318 0.00502603]\n"
     ]
    }
   ],
   "source": [
    "# change derivative if use this one\n",
    "def activationFunction(x):\n",
    "    return 1./(1+np.exp(-x)) \n",
    "\n",
    "numberOfNodes,  activationFunction, learningRate, targets, inputs, errorTolerance, maxIterations = \\\n",
    "[2,2,2], activationFunction, 0.1, [1, 0], np.array((1, 0, 1)), 1e-6, 100000\n",
    "inputs = np.array((1, 0, 1))\n",
    "\n",
    "nn3=NN(numberOfNodes, activationFunction, learningRate, targets, inputs, errorTolerance, maxIterations, test=False)\n",
    "nn3.run()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
