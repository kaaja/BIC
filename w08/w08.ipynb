{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM vs MLP\n",
    "__What advantages and disadvantages are there to support vector machines (svm) versus multilayer perceptrons (mlp)?__  <br>\n",
    "Advantage: SVM is good at classifying cases where the classes are very similar. <br>\n",
    "Disadvantege: SVM Must be used on a linearly separable problem. <br>\n",
    "SVM is slow. SVM is $\\mathcal{O}(n^2)$, while NN is $\\mathcal{O} (n)$, where $n$ is the data size.\n",
    "\n",
    "\n",
    "__What problems do they both suffer from?__ <br>\n",
    "Overfitting. Hard to decide parameter values. E.g. how many layers and neurons in NN.\n",
    "\n",
    "\n",
    "# Kernel functions\n",
    "__What is a kernel function?__ <br>\n",
    "A transormation. The goal is that the kernel function transforms data that originally is not linearly separable into a linearly separable representation that can be used with SVM.\n",
    "\n",
    "\n",
    "__Which are the most common kernel functions and roughly what kind of transformations do they correspond to?__ <br>\n",
    "Polynomial kernels that transforms the original data into higher dimensions. Radial kernels that transforms the original data into infinite dimensional spaces.\n",
    "\n",
    "# Soft Margins\n",
    "__What two factors must be balanced when using an SVM with soft margin?__ <br>\n",
    "With \"soft margin\" it is referred to the introduction of a slackness constraint in the case where the problem is not linearly seperable. The slackness constraint allows for some error in the classification. There will always be some error in the classification when applying SVM to a problem that is not linearly seperable. <br>\n",
    "\n",
    "There is a tradeoff between the error, represented in the slackness constraint, and the margin size.<br>\n",
    "\n",
    "<mark> Solution talks about overfitting: <br> \n",
    "\"An SVM with soft margin allows some of the training data to be misclassified. This is done to avoid over-fitting\n",
    "to the (perhaps faulty) training data, but we would still like to get as much of the training data as possible\n",
    "outside of the margin. This gives us two conflicting factors to worry about: Classifying training data right and\n",
    "avoid over-fitting.\" <br>\n",
    "    \n",
    "Me: I do not think soft margins has anything to do with linearly separable or not. It is about noise in the data. If the data is not linearly separable, one would use a kernel method. Still after using the kernel, there can be noise. With noise and no slackness, we will overfit the classification line to the noise. With slackness, we allow for some error. If it is the noisy terms that is inside the margin, we avoid overfitting. So more slackness gives more error, but less overfitting.\n",
    "\n",
    "# Ensemble\n",
    "__Try to come up with a few cases when using an ensemble of classifiers where it would be fine to just pick the most popular class, and where you would want to have the majority in favor of a single class or even full consensus.__\n",
    "\n",
    "Most popular:Depends on consequence of misclassification. Large costs misclassification $\\rightarrow$ majority.\n",
    "\n",
    "# Principle Component Analysis\n",
    "__What is the motivation behind principle component analysis?__ <br>\n",
    "Reducing the dimensionality. Reduction in dimensionality makes the problem less computational expensive.\n",
    "\n",
    "# Covariance\n",
    "__Work out the covariance between the x and y dimensions of the following 2-dimensional data set.\n",
    "Describe what the results indicate about the data.__\n",
    "\n",
    "| Index | 1 | 2 | 3 | 4 | 5 |\n",
    "| --- | --- | --- | --- | --- | --- |\n",
    "| x | 10 | 39 | 19 | 23 | 28 |\n",
    "| y | 43 | 13 | 32 | 21 | 20 |\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Covariance:  -120.55\n",
      "Covariance matrix numpy: \n",
      " [[ 115.7  -120.55]\n",
      " [-120.55  138.7 ]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "x = np.array((10,39,19,23,28))\n",
    "y = np.array((43, 13, 32, 21, 20))\n",
    "xMean = np.mean(x)\n",
    "yMean = np.mean(y)\n",
    "\n",
    "cov = 0#np.zeros((2,2))\n",
    "for i in range(len(x)):\n",
    "    cov += (x[i] - xMean)*(y[i] - yMean)\n",
    "cov *=1./(len(x)-1)\n",
    "print('Covariance: ',cov)\n",
    "print('Covariance matrix numpy: \\n',np.cov(x,y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$x$ and $y$ are negatively correlated. When one of the variables increaes, the other variable on average decreases."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
