{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# My notes on self organising maps\n",
    "\n",
    "## Sources\n",
    "The best description: http://davis.wpi.edu/~matt/courses/soms/\n",
    "\n",
    "Also good, with Java code: http://www.ai-junkie.com/ann/som/som1.html\n",
    "\n",
    "Figure 5 here is good showing relation date poings and weights: https://algobeans.com/2017/11/02/self-organizing-map/\n",
    "\n",
    "## How it works\n",
    "Method for dimensionality reduction into 1D or 2D plot. Re-arrange data into to clusters containing similar data. Neigboring clusters should be more simimlar than non-neigboring clusters.\n",
    "\n",
    "Type of Neural Network with one layer in addition to the inputs.\n",
    "\n",
    "A neuron is called a weight.\n",
    "\n",
    "Instances (inputs) are vectors. Example of an input can be a color. A color is represented by a 3D-vector (RGB).\n",
    "\n",
    "The number of weights can be both larger and smaller than the number of instances (inputs), see [here](https://www.researchgate.net/post/How_many_nodes_for_self-organizing_maps).\n",
    "\n",
    "A weight has two components: position (in 1D or 2D) and data-value. The data value is of the same type as the instances, e.g. 3D-vectors with RGB values.\n",
    "\n",
    "A feature is connected to every weight.\n",
    "\n",
    "__Algo:__<br>\n",
    "1) Initialize weights: Decide data value of all weights, e.g. randomly choose RGB values for all weights. <br>\n",
    "2) For every instance (input):<br>\n",
    "&emsp;    2a: Measure difference between input and all the weights, e.g. compare the RGB values of the input with the RGB values of the different weights. Example of difference measure: Euclidian. <br>\n",
    "&emsp;    2b: Choose the weight with lowest difference from the input. This weight is called the Best matching Unit (BMU).<br>\n",
    "&emsp;    2c: Adjust the weight to become more equal to the input: $w_{new} = t*input +  (1-t) w_{old}$, where $t$ is a training rate which decays with the iternations.<br>\n",
    "&emsp;    2d: Adjust the neighbors to the winning weight to become more equal to the input. The amplitude of nieghbor weight adjustment is smaller than for the winning weight and declines with distance from the winning weight.\n",
    "\n",
    "The neigbors of a weight are determined by a function that typicaly declines with iterations. Some kind of radius measure is used.\n",
    "\n",
    "The above is the main algorithm for determining the weights.\n",
    "\n",
    "Each instance (input) is located in one of the weights of the final SOM.\n",
    "\n",
    "The algorithm can create bad results. E.g. dissimilar colors can be located next to each other. For colors it is easy to detect when disimmilar colors are located next to each other. However, it might not be as easy to detect to different features located next to each other if the features are something else than coloes. A method for checking the quality of the SOM, is to calculate the average difference between every weight and the values of the weight's neighboruing weights. If the difference average difference for a weight and its neigboring weights is large, the SOM has placed dissimilar features next to each other. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# k-means clustering\n",
    "The following graph shows 2D data points. It is clear from the graphs that there are two clusters. Will \\(k\\)-means clustering be able to find these two, if we define \\(k = 2\\)? If not, why?\n",
    "\n",
    "<img src=\"two_clusters.png\" alt=\"Drawing\" style=\"width: 400px;\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Answer:__<br>\n",
    "Me: Impossible with $k=2$, since $k=2$ would produce a linear separation line. It is impossible to separate the above two clusters by a line. <br>\n",
    "\n",
    "Sol: Non-convex data set. Poins in one cluster can be closer to the other cluster center. Hence the cluseter centers are non-optimal.\n",
    "\n",
    "\n",
    "# Self organizing maps\n",
    "In SOMs, what role do the predefined topological (neighborhood) relationships between neurons in the map space play in the discovery of topological (neighborhood) relationships between input vectors in the data space?\n",
    "\n",
    "__Answer:__ <br>\n",
    "An instance (input) activates the weight that is most similar to the instance, and this weight is called the Best Matching Unit (BMU). \"activation\" of a weight implies that the weight is made more similar to the instance, increasing the probability that the same instance will activate the same weight in future iterations, since the distance is reduced. However, also the neigboring weights of the BMU are activated. Hence the probability that the neighboring weights of the BMU get activated by the same instance, or similar instances, in future iterations increaes. The result ofalso activating the neigbors of the BMU is that there will be clusters in the weight-grid representing similar instances.\n",
    "\n",
    "\n",
    "# Gaussian function in SOMs\n",
    "In SOMs, a Gaussian function, as defined below, can be used to define the neighborhood relation\n",
    "\n",
    "\\begin{equation}\n",
    "N(i,j) = e^{-\\frac{||i-j||^2}{2\\sigma^2(t)}}\n",
    "\\end{equation}\n",
    "\n",
    "where \\(i\\) and \\(j\\) are two neurons, whose position on the lattice are given by (row, columnt), and $||i-j||$ is the Euclidian distance between them. So, if \\(i\\)'s position is \\((2,2)\\) and \\(j\\)'s position is \\((3,3)\\) and $\\sigma(t) = 1$, $N(i,j)$ will be $e^{-1}$.\n",
    "\n",
    "As can also be seen, the further a neuron $i$ is from the winning neuron $j$ on the lattice the smaller $N(i,j)$ is. What would happen if $N(i,j)$ is set to and remains zero for all neurons except the winning neuron? <br>\n",
    "__Answer:__ The end-map will not contain clusteres with similar instances. The end-map will look like the instances.\n",
    "\n",
    "What would happen if $N(i,j)$ is set to and remains 1 for all neurons including the winning neuron? <br>\n",
    "__Answer:__ The network will not stabilize. Neighboring weights will always change. \n",
    "\n",
    "Why is it important to have $N(i,j)$ large for distant (on the lattice/map space) neurons in the beginning of the learning process i.e. $j$ should have a larger neighborhood, and smaller as time goes by? <br>\n",
    "__Answer:__ In the beginning, when the weights are random, the true cluster size is uncertain. A large seach radius allows for large clusters. As time goes, the largest clusters should be identidifed and the search for smaller clusters must be done through smaller search radius. Furthermore we trust the solution more as time goes.\n",
    "\n",
    "This can be controlled by $\\sigma(t)=\\sigma_0e^{-t/T}$, where $\\sigma_0$ is the initial value of $\\sigma$, and $T$ is a constant.\n",
    "\n",
    "Example Matlab script that can be run to see how the neighborhood funcion may change with time:\n",
    "<img src=\"matlab_example.png\" alt=\"Drawing\" style=\"width: 400px;\"/>\n",
    "\n",
    "Describe one other neighborhood relationship function that can be used for SOMs. In what way can it influence the learning process? Explain. <br>\n",
    "__Answer:__ Mexican hat. Pulls the closes neighbors, pushes away the intermediate neighbors and does nothing with the weigts furthest aways.\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
